import numpy as np
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import LSTM, Embedding, Dense, Dropout
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Sample conversational dataset
data = [
    ("Hello", "Hi there!"),
    ("How are you?", "I'm good, thank you!"),
    ("What is your name?", "I am a chatbot."),
    ("Tell me a joke", "Why don't scientists trust atoms? Because they make up everything!"),
    ("Goodbye", "Bye! Have a nice day!"),
]

# Preprocess dataset
questions = [q for q, a in data]
answers = [a for q, a in data]

# Tokenization and sequencing
tokenizer = Tokenizer()
tokenizer.fit_on_texts(questions + answers)
vocab_size = len(tokenizer.word_index) + 1

# Convert text to sequences
input_sequences = tokenizer.texts_to_sequences(questions)
output_sequences = tokenizer.texts_to_sequences(answers)

# Padding
max_seq_length = max(len(seq) for seq in input_sequences)
input_sequences = pad_sequences(input_sequences, maxlen=max_seq_length, padding='post')
output_sequences = pad_sequences(output_sequences, maxlen=max_seq_length, padding='post')

# Create target data with one-hot encoding
output_one_hot = tf.keras.utils.to_categorical(output_sequences, num_classes=vocab_size)

# Build the LSTM model
model = Sequential([
    Embedding(input_dim=vocab_size, output_dim=128, input_length=max_seq_length),
    LSTM(128, return_sequences=True),
    Dropout(0.2),
    Dense(vocab_size, activation='softmax'),
])

model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])

# Train the model
model.fit(input_sequences, output_one_hot, epochs=500, batch_size=32, verbose=1)

# Function to generate a response
def generate_response(input_text):
    input_seq = tokenizer.texts_to_sequences([input_text])
    input_seq = pad_sequences(input_seq, maxlen=max_seq_length, padding='post')
    predictions = model.predict(input_seq)
    response_seq = np.argmax(predictions[0], axis=1)
    response_text = tokenizer.sequences_to_texts([response_seq])[0]
    return response_text

# Chatbot interaction
print("Chatbot is ready! Type 'quit' to exit.")
while True:
    user_input = input("You: ")
    if user_input.lower() == "quit":
        print("Chatbot: Goodbye!")
        break
    response = generate_response(user_input)
    print(f"Chatbot: {response}")
